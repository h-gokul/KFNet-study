{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:  You can skip Step 1 and 2 if you do not want to run the evaluation. You can use our `output` folder from [here](https://drive.google.com/drive/u/1/folders/1cMDvOlGF1XWl98dUlTmkPmoREFFFfQ2D) which contains the estimated scene coordinates.\n",
    "\n",
    "\n",
    "# Step 1: Generate the IMAGE and LABEL list, in the ```input/``` folder\n",
    "\n",
    "\n",
    "### [UNCOMMENT BELOW CODE]: TO GENERATE LABEL LIST and IMAGE LIST\n",
    "\n",
    "- make sure the groundtruth labels are downloaded from the links in [main repository](https://github.com/zlthinker/KFNet) and stored in ```inputs/``` folder as ```$folder$-label ```\n",
    "\n",
    "- make sure the images are downloaded from the 7scenes dataset [website](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/) and saved under ``` inputs/``` as ``` $folder$-input-images```\n",
    "\n",
    "- The `label_list.txt` and the `image_list.txt` need to be saved in the ``` inputs/``` directory. Also copy the `transform.txt` from ```$folder$-label ``` and paste in ``` inputs/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# folder = 'head'\n",
    "# sequence = 'seq-01'\n",
    "\n",
    "# BIN_DIR = f\"./input/{folder}-label/{sequence}/bin\"\n",
    "# N = len(glob(f\"{BIN_DIR}/*.bin\"))\n",
    "# files = [f\"{BIN_DIR}/{str(i)}.bin\"  for i in range(N)]\n",
    "# np.savetxt('./input/label_list.txt', np.array(files), fmt='%s')\n",
    "# N = len(glob(f\"./input/{folder}-input-images/{sequence}/*.color.png\"))\n",
    "# files = sorted(glob(f\"./input/{folder}-input-images/{sequence}/*.color.png\"))\n",
    "# np.savetxt(f'./input/image_list.txt', np.array(files), fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2: Run Evaluation\n",
    "### use docker in the terminal to load to the workspace and execute evaluation.\n",
    "- download the models and place in the models folder.\n",
    "- run the evaluation script using the following command\n",
    "\n",
    "- Example:\n",
    " `python OFlowNet/eval.py --input_folder ./input --output_folder ./output/fire --model_folder ./models/OFlowNet --scene fire`\n",
    "- note the distance errors\n",
    "- save the scene coords in the output folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Visualize Optical Flow\n",
    "- specify the scene and the model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldercheck(Savepath):\n",
    "    if(not (os.path.isdir(Savepath))):\n",
    "        print(Savepath, \"  was not present, creating the folder...\")\n",
    "        os.makedirs(Savepath)\n",
    "\n",
    "def make_colorwheel():\n",
    "    \"\"\"\n",
    "    Generates a color wheel for optical flow visualization as presented in:\n",
    "        Baker et al. \"A Database and Evaluation Methodology for Optical Flow\" (ICCV, 2007)\n",
    "        URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf\n",
    "    Code follows the original C++ source code of Daniel Scharstein.\n",
    "    Code follows the the Matlab source code of Deqing Sun.\n",
    "    Returns:\n",
    "        np.ndarray: Color wheel\n",
    "    \"\"\"\n",
    "\n",
    "    RY = 15\n",
    "    YG = 6\n",
    "    GC = 4\n",
    "    CB = 11\n",
    "    BM = 13\n",
    "    MR = 6\n",
    "\n",
    "    ncols = RY + YG + GC + CB + BM + MR\n",
    "    colorwheel = np.zeros((ncols, 3))\n",
    "    col = 0\n",
    "\n",
    "    # RY\n",
    "    colorwheel[0:RY, 0] = 255\n",
    "    colorwheel[0:RY, 1] = np.floor(255*np.arange(0,RY)/RY)\n",
    "    col = col+RY\n",
    "    # YG\n",
    "    colorwheel[col:col+YG, 0] = 255 - np.floor(255*np.arange(0,YG)/YG)\n",
    "    colorwheel[col:col+YG, 1] = 255\n",
    "    col = col+YG\n",
    "    # GC\n",
    "    colorwheel[col:col+GC, 1] = 255\n",
    "    colorwheel[col:col+GC, 2] = np.floor(255*np.arange(0,GC)/GC)\n",
    "    col = col+GC\n",
    "    # CB\n",
    "    colorwheel[col:col+CB, 1] = 255 - np.floor(255*np.arange(CB)/CB)\n",
    "    colorwheel[col:col+CB, 2] = 255\n",
    "    col = col+CB\n",
    "    # BM\n",
    "    colorwheel[col:col+BM, 2] = 255\n",
    "    colorwheel[col:col+BM, 0] = np.floor(255*np.arange(0,BM)/BM)\n",
    "    col = col+BM\n",
    "    # MR\n",
    "    colorwheel[col:col+MR, 2] = 255 - np.floor(255*np.arange(MR)/MR)\n",
    "    colorwheel[col:col+MR, 0] = 255\n",
    "    return colorwheel\n",
    "\n",
    "\n",
    "def flow_uv_to_colors(u, v, convert_to_bgr=False):\n",
    "    \"\"\"\n",
    "    Applies the flow color wheel to (possibly clipped) flow components u and v.\n",
    "    According to the C++ source code of Daniel Scharstein\n",
    "    According to the Matlab source code of Deqing Sun\n",
    "    Args:\n",
    "        u (np.ndarray): Input horizontal flow of shape [H,W]\n",
    "        v (np.ndarray): Input vertical flow of shape [H,W]\n",
    "        convert_to_bgr (bool, optional): Convert output image to BGR. Defaults to False.\n",
    "    Returns:\n",
    "        np.ndarray: Flow visualization image of shape [H,W,3]\n",
    "    \"\"\"\n",
    "    flow_image = np.zeros((u.shape[0], u.shape[1], 3), np.uint8)\n",
    "    colorwheel = make_colorwheel()  # shape [55x3]\n",
    "    ncols = colorwheel.shape[0]\n",
    "    rad = np.sqrt(np.square(u) + np.square(v))\n",
    "    a = np.arctan2(-v, -u)/np.pi\n",
    "    fk = (a+1) / 2*(ncols-1)\n",
    "    k0 = np.floor(fk).astype(np.int32)\n",
    "    k1 = k0 + 1\n",
    "    k1[k1 == ncols] = 0\n",
    "    f = fk - k0\n",
    "    for i in range(colorwheel.shape[1]):\n",
    "        tmp = colorwheel[:,i]\n",
    "        col0 = tmp[k0] / 255.0\n",
    "        col1 = tmp[k1] / 255.0\n",
    "        col = (1-f)*col0 + f*col1\n",
    "        idx = (rad <= 1)\n",
    "        col[idx]  = 1 - rad[idx] * (1-col[idx])\n",
    "        col[~idx] = col[~idx] * 0.75   # out of range\n",
    "        # Note the 2-i => BGR instead of RGB\n",
    "        ch_idx = 2-i if convert_to_bgr else i\n",
    "        flow_image[:,:,ch_idx] = np.floor(255 * col)\n",
    "    return flow_image\n",
    "\n",
    "\n",
    "def visualize(flow_uv, clip_flow=None, convert_to_bgr=False):\n",
    "    \"\"\"\n",
    "    Expects a two dimensional flow image of shape.\n",
    "    Args:\n",
    "        flow_uv (np.ndarray): Flow UV image of shape [H,W,2]\n",
    "        clip_flow (float, optional): Clip maximum of flow values. Defaults to None.\n",
    "        convert_to_bgr (bool, optional): Convert output image to BGR. Defaults to False.\n",
    "    Returns:\n",
    "        np.ndarray: Flow visualization image of shape [H,W,3]\n",
    "    \"\"\"\n",
    "    assert flow_uv.ndim == 3, 'input flow must have three dimensions'\n",
    "    assert flow_uv.shape[2] == 2, 'input flow must have shape [H,W,2]'\n",
    "    \n",
    "    if clip_flow is not None:\n",
    "        flow_uv = np.clip(flow_uv, 0, clip_flow)\n",
    "    u = flow_uv[:,:,0]\n",
    "    v = flow_uv[:,:,1]\n",
    "    rad = np.sqrt(np.square(u) + np.square(v))\n",
    "    rad_max = np.max(rad)\n",
    "    epsilon = 1e-5\n",
    "    u = u / (rad_max + epsilon)\n",
    "    v = v / (rad_max + epsilon)\n",
    "    return flow_uv_to_colors(u, v, convert_to_bgr)\n",
    "    \n",
    "def plotQuivers(image_file1, image_file2, flow_file, thres=100):\n",
    "    image1 = cv2.imread(image_file1)\n",
    "    image2 = cv2.imread(image_file2)\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    flow_uv = np.load(flow_file)\n",
    "\n",
    "    blend_image = cv2.addWeighted(image1, 0.5, image2, 0.5, 0)\n",
    "\n",
    "    height, width, _ = flow_uv.shape\n",
    "    # print( flow_uv.shape)\n",
    "    for h in range(2, height-2, 3):\n",
    "        for w in range(2, width-2, 3):\n",
    "            confidence = flow_uv[h, w, 2]\n",
    "            if confidence < thres:\n",
    "                continue\n",
    "            u = flow_uv[h, w, 0]\n",
    "            v = flow_uv[h, w, 1]\n",
    "\n",
    "            x = w * 8 + 4\n",
    "            y = h * 8 + 4\n",
    "            cv2.line(blend_image, (x, y), (int(x+u*8), int(y+v*8)), (0, 255, 0), 2)\n",
    "    return blend_image\n",
    "\n",
    "def visualize_opticalflow(flow):\n",
    "    confidence = flow[..., 2]        \n",
    "    flow = flow[..., :2]\n",
    "    uncertainity = np.dstack((confidence,confidence,confidence))\n",
    "    flowrgb = visualize(flow)\n",
    "    return flowrgb, uncertainity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'fire'\n",
    "N = len(glob(f\"output/{folder}/flow/*.npy\"))-1\n",
    "flowfiles = [f'output/{folder}/flow/{str(i)}_{str(i+1)}_flow.npy' for i in range(N)]\n",
    "imagefiles = sorted(glob(f'input/{folder}-input-images/seq-01/*.color.png'))\n",
    "print(len(imagefiles), len(flowfiles))\n",
    "\n",
    "for i in range( len(imagefiles)-2 ):\n",
    "    quiverplot = plotQuivers(imagefiles[i], imagefiles[i+1], flowfiles[i])\n",
    "    quiverplot=cv2.cvtColor(quiverplot, cv2.COLOR_RGB2BGR)   \n",
    "    flowrgb, uncertainity = visualize_opticalflow(np.load(flowfiles[i]))\n",
    "    uncertainity =  cv2.normalize(uncertainity, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    out = np.vstack(( quiverplot, np.hstack(  (cv2.resize(flowrgb, (flowrgb.shape[1]*4, flowrgb.shape[0]*4)),\\\n",
    "        cv2.resize(uncertainity, (uncertainity.shape[1]*4, uncertainity.shape[0]*4)))  ))) \n",
    "    out = out.astype(np.uint8)\n",
    "    if(i==0):\n",
    "        h,w = out.shape[:2]\n",
    "        foldercheck(f'./output/{folder}/')    \n",
    "        writer = cv2.VideoWriter(f'./output/{folder}_video.mp4', cv2.VideoWriter_fourcc('m', 'p', '4', 'v'), 20, (w, h))    \n",
    "    writer.write(out)\n",
    "\n",
    "writer.release()\n",
    "print(\"Done stitching\")\n",
    "print(f\"Check './output/' {folder}_video.mp4\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
